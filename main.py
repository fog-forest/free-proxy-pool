import time

from config.proxy_sources import NORMAL_PROXIES, ANONYMOUS_PROXIES
from utils.crawler import ProxyCrawler
from utils.storage import ProxyStorage
from utils.validator import ProxyValidator


def main():
    # è¾“å…¥äº¤äº’
    print("=" * 60)
    print("ğŸ“Œ ä»£ç†IPçˆ¬å–éªŒè¯ç¨‹åº - é…ç½®å‘å¯¼")
    print("=" * 60)
    check_hours = input("è¯·è¾“å…¥ä»£ç†å¾ªç¯æ£€æŸ¥é—´éš”æ—¶é—´ï¼ˆå°æ—¶ï¼Œé»˜è®¤2hï¼‰ï¼š").strip()
    check_hours = int(check_hours) if check_hours and check_hours.isdigit() else 2
    check_interval = check_hours * 3600

    # ä»£ç†ç±»å‹é€‰æ‹©
    proxy_type = input("è¯·é€‰æ‹©çˆ¬å–çš„ä»£ç†ç±»å‹ï¼ˆall-å…¨éƒ¨/normal-æ™®é€š/anonymous-é«˜åŒ¿ï¼Œé»˜è®¤allï¼‰ï¼š").strip().lower() or "all"
    while proxy_type not in ["all", "normal", "anonymous"]:
        print("âŒ é”™è¯¯ï¼šä»…æ”¯æŒ all/normal/anonymous ä¸‰ç§è¾“å…¥ï¼")
        proxy_type = input("è¯·é‡æ–°è¾“å…¥ä»£ç†ç±»å‹ï¼š").strip().lower() or "all"

    thread_count = 1000  # çº¿ç¨‹æ•°å¯é€‚å½“è°ƒæ•´
    test_config = {
        "url": "http://captive.apple.com/",
        "keyword": "Success",
        "encoding": "utf-8"
    }

    # å¯åŠ¨ä¿¡æ¯
    print("\n" + "=" * 80)
    print("ğŸš€ ä»£ç†IPçˆ¬å–éªŒè¯ç¨‹åº å¯åŠ¨æˆåŠŸ")
    print("=" * 80)
    print(f"ğŸ“‹ æ ¸å¿ƒé…ç½®ï¼š")
    print(f"   â”œâ”€ çˆ¬å–ç±»å‹ï¼š{proxy_type}ï¼ˆall=å…¨éƒ¨ / normal=æ™®é€š / anonymous=é«˜åŒ¿ï¼‰")
    print(f"   â”œâ”€ æ£€æŸ¥é—´éš”ï¼š{check_hours} å°æ—¶ï¼ˆ{check_interval} ç§’ï¼‰")
    print(f"   â”œâ”€ éªŒè¯çº¿ç¨‹æ•°ï¼š{thread_count} ä¸ª")
    print(f"   â””â”€ æµ‹è¯•URLï¼š{test_config['url']}")
    print("=" * 80 + "\n")

    while True:
        # åˆå§‹åŒ–çˆ¬è™«å®ä¾‹
        crawler_normal = ProxyCrawler() if proxy_type in ["all", "normal"] else None
        crawler_anonymous = ProxyCrawler() if proxy_type in ["all", "anonymous"] else None

        # çˆ¬å–æ™®é€šä»£ç†
        normal_proxies = []
        if proxy_type in ["all", "normal"]:
            print("ğŸ“¥ [é˜¶æ®µ1/3] å¼€å§‹çˆ¬å– æ™®é€šä»£ç† IP...")
            print("-" * 50)
            for source in NORMAL_PROXIES:
                crawler_normal.crawl(source)
            print("\nğŸ” [æ™®é€šä»£ç†] å¼€å§‹å»é‡...")
            normal_proxies = crawler_normal.get_unique_proxies()
            print(f"âœ… [æ™®é€šä»£ç†] æœ€ç»ˆå¯ç”¨IPæ•°ï¼š{len(normal_proxies)} ä¸ª\n")

        # çˆ¬å–é«˜åŒ¿ä»£ç†
        anonymous_proxies = []
        if proxy_type in ["all", "anonymous"]:
            print("ğŸ“¥ [é˜¶æ®µ1/3] å¼€å§‹çˆ¬å– é«˜åŒ¿ä»£ç† IP...")
            print("-" * 50)
            for source in ANONYMOUS_PROXIES:
                crawler_anonymous.crawl(source)
            print("\nğŸ” [é«˜åŒ¿ä»£ç†] å¼€å§‹å»é‡...")
            anonymous_proxies = crawler_anonymous.get_unique_proxies()
            print(f"âœ… [é«˜åŒ¿ä»£ç†] æœ€ç»ˆå¯ç”¨IPæ•°ï¼š{len(anonymous_proxies)} ä¸ª\n")

        # åˆå¹¶ä»£ç†å¹¶ç»Ÿè®¡
        proxy_with_type = []
        for ip in normal_proxies:
            proxy_with_type.append((ip, "normal"))
        for ip in anonymous_proxies:
            proxy_with_type.append((ip, "anonymous"))
        all_proxies = [item[0] for item in proxy_with_type]

        # çˆ¬å–ç»“æœæ±‡æ€»
        print("=" * 60)
        print("ğŸ“Š çˆ¬å–ç»“æœæ±‡æ€»")
        print("=" * 60)
        print(f"   â”œâ”€ æ™®é€šä»£ç†ï¼š{len(normal_proxies):3d} ä¸ª")
        print(f"   â”œâ”€ é«˜åŒ¿ä»£ç†ï¼š{len(anonymous_proxies):3d} ä¸ª")
        print(f"   â””â”€ æ€»å¾…éªŒè¯ï¼š{len(all_proxies):3d} ä¸ª")
        print("=" * 60 + "\n")

        # æ‰¹é‡éªŒè¯
        valid_normal, valid_anonymous = [], []
        if all_proxies:
            print("ğŸ” [é˜¶æ®µ2/3] å¼€å§‹æ‰¹é‡éªŒè¯ä»£ç†æœ‰æ•ˆæ€§...")
            print(f"â„¹ï¸  éªŒè¯é…ç½®ï¼šçº¿ç¨‹æ•°={thread_count} | æµ‹è¯•URL={test_config['url']}")
            validator = ProxyValidator()
            valid_proxies = validator.validate(
                all_proxies,
                test_config["url"],
                test_config["keyword"],
                thread_count
            )

            # æŒ‰ç±»å‹æ‹†åˆ†æœ‰æ•ˆä»£ç†
            valid_set = set(valid_proxies)
            for ip, type_tag in proxy_with_type:
                if ip in valid_set:
                    if type_tag == "normal":
                        valid_normal.append(ip)
                    else:
                        valid_anonymous.append(ip)

            # å»é‡å†²çªï¼ˆä¼˜å…ˆä¿ç•™é«˜åŒ¿ï¼‰
            valid_normal = [ip for ip in valid_normal if ip not in valid_anonymous]

            # éªŒè¯ç»“æœæ˜¾ç¤º
            print(f"\nâœ… [éªŒè¯å®Œæˆ]")
            print(f"   â”œâ”€ æ€»å¾…éªŒè¯ï¼š{len(all_proxies):3d} ä¸ª")
            print(f"   â”œâ”€ æœ‰æ•ˆä»£ç†ï¼š{len(valid_proxies):3d} ä¸ª")
            print(f"   â”œâ”€ æœ‰æ•ˆç‡ï¼š{(len(valid_proxies) / len(all_proxies) * 100):6.2f}%")
            print(f"   â”œâ”€ æœ‰æ•ˆæ™®é€šä»£ç†ï¼š{len(valid_normal):3d} ä¸ªï¼ˆç¤ºä¾‹ï¼š{valid_normal[:2]}ï¼‰")
            print(f"   â””â”€ æœ‰æ•ˆé«˜åŒ¿ä»£ç†ï¼š{len(valid_anonymous):3d} ä¸ªï¼ˆç¤ºä¾‹ï¼š{valid_anonymous[:2]}ï¼‰")
        else:
            print("âš ï¸ [é˜¶æ®µ2/3] æ— å¾…éªŒè¯çš„ä»£ç†IPï¼Œè·³è¿‡éªŒè¯æ­¥éª¤")

        # ä¿å­˜ç»“æœ
        print("\n" + "=" * 60)
        print("ğŸ’¾ [é˜¶æ®µ3/3] ä¿å­˜æœ‰æ•ˆä»£ç†IP...")
        ProxyStorage.save_proxies_with_type(
            filename="proxy_ip.json",
            normal_proxies=valid_normal,
            anonymous_proxies=valid_anonymous
        )

        # ä¿å­˜ç»“æœæ±‡æ€»
        total_valid = len(valid_normal) + len(valid_anonymous)
        print(f"âœ… ä¿å­˜å®Œæˆï¼")
        print(f"   â”œâ”€ ä¿å­˜æ–‡ä»¶ï¼šproxy_ip.json")
        print(f"   â”œâ”€ æœ‰æ•ˆæ™®é€šä»£ç†ï¼š{len(valid_normal):3d} ä¸ª")
        print(f"   â”œâ”€ æœ‰æ•ˆé«˜åŒ¿ä»£ç†ï¼š{len(valid_anonymous):3d} ä¸ª")
        print(f"   â””â”€ æ€»è®¡æœ‰æ•ˆä»£ç†ï¼š{total_valid:3d} ä¸ª")
        print("=" * 60)

        # ä¸‹ä¸€è½®æç¤º
        next_round = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(time.time() + check_interval))
        print(f"\nâ° æœ¬è½®ä»»åŠ¡å®Œæˆï¼{check_hours}å°æ—¶åï¼ˆé¢„è®¡ {next_round}ï¼‰å¼€å§‹ä¸‹ä¸€è½®æ£€æŸ¥")
        print("-" * 80 + "\n")
        time.sleep(check_interval)


if __name__ == "__main__":
    main()
