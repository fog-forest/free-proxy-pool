import json
import re
import time
from typing import List, Dict, Optional

import requests
from bs4 import BeautifulSoup


class ProxyCrawler:
    """ä»£ç†IPçˆ¬è™«ç±»"""

    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36"
        }
        self.proxies = []  # å­˜å‚¨çˆ¬å–çš„ä»£ç†IP

    def fetch(self, url: str, timeout: int = 5, post_data: Optional[Dict] = None) -> str:
        """è¯·æ±‚é¡µé¢ï¼Œè¿”å›æºç ï¼ˆæ”¯æŒGET/POSTï¼‰"""
        try:
            if post_data:
                response = requests.post(url, headers=self.headers, data=post_data, timeout=timeout)
            else:
                response = requests.get(url, headers=self.headers, timeout=timeout)
            return response.text
        except Exception as e:
            print(f"âŒ [è¯·æ±‚å¤±è´¥] {url}ï¼š{str(e)[:50]}")
            return ""

    @staticmethod
    def parse_api1(html: str) -> List[str]:
        """è§£æAPIå“åº”ï¼Œæå–IP:PORTæ ¼å¼"""
        return re.findall(r'(\d+\.\d+\.\d+\.\d+:\d+)', html)

    @staticmethod
    def parse_api2(html: str) -> List[str]:
        """è§£æJSONæ¥å£ï¼Œæå–protocol=1/2çš„IP+Port"""
        result = []
        try:
            data = json.loads(html)
            proxy_list = []
            # å…¼å®¹å¤šç§JSONæ•°æ®ç»“æ„ï¼šdata.data.listã€data.listã€ç›´æ¥æ˜¯åˆ—è¡¨
            if isinstance(data.get("data"), dict):
                proxy_list = data["data"].get("list", [])
            elif isinstance(data, list):
                proxy_list = data
            elif isinstance(data.get("list"), list):
                proxy_list = data["list"]

            if not proxy_list:
                print("âš ï¸ [JSONæ¥å£] æ— ä»£ç†æ•°æ®")
                return result

            valid_protocol = {1, 2}
            for proxy in proxy_list:
                protocol = proxy.get("protocol")
                ip = proxy.get("ip")
                port = proxy.get("port")
                protocol_valid = protocol in valid_protocol
                if protocol_valid and ip is not None and port is not None:
                    result.append(f"{str(ip).strip()}:{str(port).strip()}")

            return result
        except json.JSONDecodeError:
            print("âŒ [JSONè§£æ] éæœ‰æ•ˆJSONæ ¼å¼")
            return result
        except Exception as e:
            print(f"âŒ [JSONè§£æ] å¼‚å¸¸ï¼š{str(e)[:50]}")
            return result

    @staticmethod
    def parse_article1(html: str) -> List[str]:
        """è§£ææ–‡ç« é¡µé¢ï¼Œæå–IP:PORTæ ¼å¼"""
        return re.findall(r'(\d+\.\d+\.\d+\.\d+:\d+)', html)

    @staticmethod
    def parse_html1(html: str) -> List[str]:
        """è§£æHTMLï¼ŒIP+ç«¯å£åœ¨åŒä¸€ä¸ªtdæ ‡ç­¾"""
        soup = BeautifulSoup(html, 'html5lib')
        trs = soup.find_all('tr')
        return [tr.find_all('td')[0].text.strip() for tr in trs[1:]]

    @staticmethod
    def parse_html2(html: str) -> List[str]:
        """è§£æHTMLï¼ŒIPåœ¨ç¬¬1ä¸ªtdï¼Œç«¯å£åœ¨ç¬¬2ä¸ªtd"""
        soup = BeautifulSoup(html, 'html5lib')
        trs = soup.find_all('tr')
        return [f"{tr.find_all('td')[0].text.strip()}:{tr.find_all('td')[1].text.strip()}"
                for tr in trs[1:]]

    @staticmethod
    def parse_html3(html: str) -> List[str]:
        """è§£æHTMLï¼ŒIPåœ¨ç¬¬2ä¸ªtdï¼Œç«¯å£åœ¨ç¬¬3ä¸ªtd"""
        soup = BeautifulSoup(html, 'html5lib')
        trs = soup.find_all('tr')
        return [f"{tr.find_all('td')[1].text.strip()}:{tr.find_all('td')[2].text.strip()}"
                for tr in trs[1:]]

    @staticmethod
    def parse_html4(html: str) -> List[str]:
        """è§£æHTMLï¼ŒIPåœ¨ç¬¬1ä¸ªthï¼Œç«¯å£åœ¨ç¬¬2ä¸ªth"""
        soup = BeautifulSoup(html, 'html5lib')
        trs = soup.find_all('tr')
        return [f"{tr.find_all('th')[0].text.strip()}:{tr.find_all('th')[1].text.strip()}"
                for tr in trs[1:]]

    @staticmethod
    def parse_fpslist(html: str) -> List[str]:
        """è§£æfpsListæ ¼å¼JSONï¼Œæå–ip+port"""
        pattern = r'const fpsList = (\[.*?\]);'
        match = re.search(pattern, html, re.DOTALL)
        if not match:
            print("âš ï¸ [fpsListè§£æ] æœªæ‰¾åˆ°æ•°æ®")
            return []

        try:
            proxy_list = json.loads(match.group(1))
            result = []
            for item in proxy_list:
                if "ip" in item and "port" in item:
                    result.append(f"{item['ip']}:{item['port']}")

            return result
        except json.JSONDecodeError as e:
            print(f"âŒ [fpsListè§£æ] JSONé”™è¯¯ï¼š{str(e)[:50]}")
            return []

    @staticmethod
    def parse_fineproxy(html: str) -> List[str]:
        """è§£æFineProxyçš„å“åº”æ•°æ®, æå–ip+port"""
        try:
            response_json = json.loads(html)
        except json.JSONDecodeError as e:
            print(f"âŒ [FineProxyè§£æ] JSONé”™è¯¯ï¼š{str(e)[:50]}")
            return []

        rows_html = response_json.get("data", {}).get("rows", "").strip()
        if not rows_html:
            print("âš ï¸ [FineProxyè§£æ] æ— æœ‰æ•ˆæ•°æ®ï¼ˆrowsä¸ºç©ºï¼‰")
            return []

        pattern = r'<td\s+class=["\']table-ip["\']\s*>\s*(\d+\.\d+\.\d+\.\d+)\s*</td>\s*<td\s*>\s*(\d+)\s*</td>'
        matches = re.findall(pattern, rows_html, re.IGNORECASE | re.DOTALL)
        if not matches:
            print("âš ï¸ [FineProxyè§£æ] æœªåŒ¹é…åˆ°IPå’Œç«¯å£")
            return []

        result = []
        for ip, port in matches:
            port_stripped = port.strip()
            if port_stripped:
                result.append(f"{ip}:{port_stripped}")

        return result

    def _get_auto_page_count(self, url: str) -> int:
        """å†…éƒ¨æ–¹æ³•ï¼šè‡ªåŠ¨è·å–api2æ€»é¡µæ•°"""
        first_page_url = url + "1" if url.endswith("page=") else url
        html = self.fetch(first_page_url)
        if not html:
            print("âŒ [è‡ªåŠ¨åˆ†é¡µ] è·å–æ€»é¡µæ•°å¤±è´¥")
            return 1
        try:
            data = json.loads(html)
            page_count = 0
            if isinstance(data.get("data"), dict):
                page_count = data["data"].get("page_count",
                                              data["data"].get("total_pages",
                                                               data["data"].get("total_page", 0)))
            page_count = page_count or data.get("page_count", 0)
            return page_count if page_count > 0 else 1
        except Exception:
            print("âŒ [è‡ªåŠ¨åˆ†é¡µ] è§£ææ€»é¡µæ•°å¤±è´¥")
            return 1

    def _is_valid_proxy(self, proxy: str) -> bool:
        """å†…éƒ¨æ–¹æ³•ï¼šæ ¡éªŒä»£ç†IP:PORTçš„åˆæ³•æ€§"""
        if not proxy or ':' not in proxy:
            return False
        ip, port_str = proxy.split(':', 1)  # åˆ†å‰²IPå’Œç«¯å£ï¼ˆä»…åˆ†å‰²ä¸€æ¬¡ï¼Œé¿å…ç«¯å£å«:ï¼‰

        # 1. IPåˆæ³•æ€§æ ¡éªŒï¼ˆIPv4ï¼šå››æ®µæ•°å­—ï¼Œæ¯æ®µ0-255ï¼‰
        ip_segments = ip.split('.')
        if len(ip_segments) != 4:
            return False
        try:
            ip_nums = [int(seg) for seg in ip_segments]
            if not all(0 <= num <= 255 for num in ip_nums):
                return False
        except ValueError:
            return False

        # 2. ç«¯å£åˆæ³•æ€§æ ¡éªŒï¼ˆ1-65535ï¼Œæ”¯æŒæ•°å­—å­—ç¬¦ä¸²ï¼‰
        try:
            port = int(port_str.strip())
            if not (1 <= port <= 65535):
                return False
        except ValueError:
            return False

        return True

    def crawl(self, source: dict) -> int:
        """çˆ¬å–æŒ‡å®šæºçš„ä»£ç†IP"""
        parser = getattr(self, f"parse_{source['parser']}", None)
        if not parser:
            print(f"âŒ [çˆ¬è™«é”™è¯¯] æœªçŸ¥è§£æå™¨ï¼š{source['parser']}")
            return 0

        # åˆ†éš”ç¬¦
        source_name = source['name'].center(30, ' ')
        print(f"\n{'=' * 80}")
        print(f"ğŸ“¥ å¼€å§‹çˆ¬å– | {source_name}")
        print(f"{'=' * 80}")

        temp_proxies = []
        no_data_count = 0
        before_count = len(self.proxies)
        try:
            # å¤„ç†åˆ†é¡µé…ç½®
            if source['pages'] == "auto":
                if source['parser'] != "api2":
                    print("âš ï¸ [åˆ†é¡µè­¦å‘Š] ä»…api2æ”¯æŒè‡ªåŠ¨åˆ†é¡µï¼Œé»˜è®¤çˆ¬1é¡µ")
                    total_pages = 1
                else:
                    total_pages = self._get_auto_page_count(source['url'])
                    print(f"â„¹ï¸ [åˆ†é¡µä¿¡æ¯] è‡ªåŠ¨è·å–æ€»é¡µæ•°ï¼š{total_pages} é¡µ")
            else:
                total_pages = source['pages']
                print(f"â„¹ï¸ [åˆ†é¡µä¿¡æ¯] é…ç½®çˆ¬å–é¡µæ•°ï¼š{total_pages} é¡µ")

            # åˆ†é¡µçˆ¬å–
            for current_page in range(1, total_pages + 1):
                # æ„å»ºè¯·æ±‚URL
                if "api1" in source['parser']:
                    url = source['url']
                elif source['parser'] in ["fineproxy"]:
                    url = source['url']
                elif any(domain in source['url'] for domain in ["kxdaili.com", "qiyunip.com"]):
                    url = f"{source['url']}{current_page}.html"
                else:
                    url = f"{source['url']}{current_page}"

                # å¤„ç†POSTæ•°æ®
                post_data = source.get("body") or None
                if post_data and isinstance(post_data, dict):
                    post_data = {k: v.replace("{page}", str(current_page)) if "{page}" in str(v) else v
                                 for k, v in post_data.items()}

                # åˆ†é¡µæ—¥å¿—
                print(f"\nğŸ”„ æ­£åœ¨çˆ¬å– | é¡µç ï¼š{current_page:2d}/{total_pages:2d} | URLï¼š{url}")
                print(f"â„¹ï¸  å½“å‰ç´¯è®¡ | æ€»åˆ—è¡¨IPæ•°ï¼š{len(self.proxies):3d} ä¸ª")

                html = self.fetch(url, post_data=post_data)

                # ä»…åœ¨ç¬¬1é¡µæ˜¾ç¤ºé¢„è§ˆ
                if current_page == 1 and html:
                    preview = html[:500].replace('\n', ' ').strip()  # å»é™¤æ¢è¡Œï¼Œç²¾ç®€æ˜¾ç¤º
                    print(f"ğŸ“„ å“åº”é¢„è§ˆï¼š{preview}...")

                if not html:
                    print(f"âš ï¸  é¡µç  {current_page:2d} | è¯·æ±‚å¤±è´¥ï¼Œè·³è¿‡")
                    no_data_count += 1
                else:
                    ips = parser(html)
                    valid_ips = [ip for ip in ips if self._is_valid_proxy(ip)]  # æ ¡éªŒç»“æœ
                    page_valid_count = len(valid_ips)

                    # åˆ†é¡µç»“æœæ—¥å¿—
                    print(f"âœ…  é¡µç  {current_page:2d} | æå–IPï¼š{len(ips):2d} ä¸ª | æœ‰æ•ˆæ ¼å¼ï¼š{page_valid_count:2d} ä¸ª")

                    temp_proxies.extend(valid_ips)
                    no_data_count = 0 if page_valid_count > 0 else no_data_count + 1

                    if page_valid_count == 0:
                        print(f"âš ï¸  é¡µç  {current_page:2d} | æ— æœ‰æ•ˆIPï¼Œè¿ç»­æ— æ•°æ®æ¬¡æ•°ï¼š{no_data_count}")

                # è¿ç»­3æ¬¡æ— æ•°æ®åœæ­¢
                if no_data_count >= 3:
                    print(f"\nğŸ›‘ [çˆ¬å–åœæ­¢] è¿ç»­3æ¬¡æ— æœ‰æ•ˆIPï¼Œæå‰ç»“æŸå½“å‰æºçˆ¬å–")
                    break

                time.sleep(source['delay'])

            # çˆ¬å–å®Œæˆç»Ÿè®¡
            self.proxies.extend(temp_proxies)
            crawl_count = len(temp_proxies)
            after_count = len(self.proxies)
            print(f"\n{'=' * 80}")
            print(f"âœ… çˆ¬å–å®Œæˆ | {source['name']}")
            print(f"   â”œâ”€ æœ¬æºæ€§æ–°å¢æœ‰æ•ˆIPï¼š{crawl_count:3d} ä¸ª")
            print(f"   â”œâ”€ æ€»åˆ—è¡¨ç´¯è®¡IPæ•°ï¼š{after_count:3d} ä¸ª")
            print(f"   â””â”€ æœ¬æ¬¡æ–°å¢é‡ï¼š{after_count - before_count:3d} ä¸ª")
            print(f"{'=' * 80}")

        except Exception as e:
            print(f"\n{'=' * 80}")
            print(f"âŒ çˆ¬å–å¤±è´¥ | {source['name']}")
            print(f"   â”œâ”€ å¤±è´¥åŸå› ï¼š{str(e)[:50]}")
            valid_temp = [ip for ip in temp_proxies if re.match(r'\d+\.\d+\.\d+\.\d+:\d+', ip)]
            crawl_count = len(valid_temp)
            if valid_temp:
                self.proxies.extend(valid_temp)
                after_count = len(self.proxies)
                print(f"   â”œâ”€ å¼‚å¸¸æ¢å¤ï¼šå·²ç´¯è®¡æœ‰æ•ˆIP {crawl_count:3d} ä¸ª")
                print(f"   â””â”€ æ€»åˆ—è¡¨å½“å‰ç´¯è®¡ï¼š{after_count:3d} ä¸ª")
            print(f"{'=' * 80}")

        return crawl_count

    def get_unique_proxies(self) -> List[str]:
        """ä»£ç†IPå»é‡ï¼ˆä¼˜åŒ–æ—¥å¿—æ˜¾ç¤ºï¼‰"""
        before_count = len(self.proxies)
        unique_list = list(set(self.proxies))
        after_count = len(unique_list)
        duplicate_count = before_count - after_count
        self.proxies.clear()

        # è®¡ç®—é‡å¤ç‡ï¼ˆå¤„ç†é™¤ä»¥0ï¼‰
        duplicate_rate = (duplicate_count / before_count) * 100 if before_count != 0 else 0.0

        # ä¼˜åŒ–å»é‡æ—¥å¿—æ ¼å¼
        print(f"\n{'=' * 60}")
        print(f"ğŸ” ä»£ç†IPå»é‡ç»Ÿè®¡")
        print(f"   â”œâ”€ å»é‡å‰æ€»æ•°é‡ï¼š{before_count:3d} ä¸ª")
        print(f"   â”œâ”€ å»é‡åæ€»æ•°é‡ï¼š{after_count:3d} ä¸ª")
        print(f"   â”œâ”€ é‡å¤IPæ•°é‡ï¼š{duplicate_count:3d} ä¸ª")
        print(f"   â””â”€ é‡å¤ç‡ï¼š{duplicate_rate:6.2f}%")
        print(f"{'=' * 60}")

        return unique_list
