import json
import re
import time
from typing import List

import requests
from bs4 import BeautifulSoup


class ProxyCrawler:
    """ä»£ç†IPçˆ¬è™«ç±»"""

    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36"
        }
        self.proxies = []  # å­˜å‚¨çˆ¬å–çš„ä»£ç†IP

    def fetch(self, url: str, timeout: int = 5) -> str:
        """è¯·æ±‚é¡µé¢ï¼Œè¿”å›æºç """
        try:
            response = requests.get(url, headers=self.headers, timeout=timeout)
            return response.text
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¤±è´¥ {url}ï¼š{str(e)[:50]}")
            return ""

    @staticmethod
    def parse_api1(html: str) -> List[str]:
        """è§£æAPIå“åº”ï¼Œæå–IP:PORTæ ¼å¼"""
        return re.findall(r'(\d+\.\d+\.\d+\.\d+:\d+)', html)

    @staticmethod
    def parse_api2(html: str) -> List[str]:
        """è§£æJSONæ¥å£ï¼Œæå–protocol=1/2çš„IP+Port"""
        result = []
        try:
            data = json.loads(html)

            # å…¼å®¹ä¸åŒJSONç»“æ„æå–ä»£ç†åˆ—è¡¨
            proxy_list = []
            if isinstance(data.get("data"), dict):
                proxy_list = data["data"].get("list", [])
            elif isinstance(data, list):
                proxy_list = data
            elif isinstance(data.get("list"), list):
                proxy_list = data["list"]

            if not proxy_list:
                print("âš ï¸ JSONæ¥å£æ— ä»£ç†æ•°æ®")
                return result

            # è¿‡æ»¤æœ‰æ•ˆä»£ç†
            valid_protocol = {1, 2}
            total = len(proxy_list)
            filtered = 0

            for proxy in proxy_list:
                protocol = proxy.get("protocol")
                ip = proxy.get("ip")
                port = proxy.get("port")

                # æ ¡éªŒIP/ç«¯å£/åè®®åˆæ³•æ€§
                ip_valid = isinstance(ip, str) and re.match(r'\d+\.\d+\.\d+\.\d+', ip)
                port_valid = isinstance(port, (int, str)) and str(port).isdigit()
                protocol_valid = protocol in valid_protocol

                if ip_valid and port_valid and protocol_valid:
                    result.append(f"{ip}:{str(port).strip()}")
                    print(f"{ip}:{str(port).strip()}")
                    filtered += 1

            print(f"âœ… JSONè§£æå®Œæˆ | æ€»æ•°ï¼š{total} | æœ‰æ•ˆï¼š{filtered}")
            return result

        except json.JSONDecodeError:
            print("âŒ éæœ‰æ•ˆJSONæ ¼å¼")
            return result
        except Exception as e:
            print(f"âŒ JSONè§£æå¼‚å¸¸ï¼š{str(e)[:50]}")
            return result

    @staticmethod
    def parse_article1(html: str) -> List[str]:
        """è§£ææ–‡ç« é¡µé¢ï¼Œæå–IP:PORTæ ¼å¼"""
        return re.findall(r'(\d+\.\d+\.\d+\.\d+:\d+)', html)

    @staticmethod
    def parse_html1(html: str) -> List[str]:
        """è§£æHTMLï¼ŒIP+ç«¯å£åœ¨åŒä¸€ä¸ªtdæ ‡ç­¾"""
        soup = BeautifulSoup(html, 'html5lib')
        trs = soup.find_all('tr')
        return [tr.find_all('td')[0].text.strip() for tr in trs[1:]]

    @staticmethod
    def parse_html2(html: str) -> List[str]:
        """è§£æHTMLï¼ŒIPåœ¨ç¬¬1ä¸ªtdï¼Œç«¯å£åœ¨ç¬¬2ä¸ªtd"""
        soup = BeautifulSoup(html, 'html5lib')
        trs = soup.find_all('tr')
        return [f"{tr.find_all('td')[0].text.strip()}:{tr.find_all('td')[1].text.strip()}"
                for tr in trs[1:]]

    @staticmethod
    def parse_html3(html: str) -> List[str]:
        """è§£æHTMLï¼ŒIPåœ¨ç¬¬2ä¸ªtdï¼Œç«¯å£åœ¨ç¬¬3ä¸ªtd"""
        soup = BeautifulSoup(html, 'html5lib')
        trs = soup.find_all('tr')
        return [f"{tr.find_all('td')[1].text.strip()}:{tr.find_all('td')[2].text.strip()}"
                for tr in trs[1:]]

    @staticmethod
    def parse_html4(html: str) -> List[str]:
        """è§£æHTMLï¼ŒIPåœ¨ç¬¬1ä¸ªthï¼Œç«¯å£åœ¨ç¬¬2ä¸ªth"""
        soup = BeautifulSoup(html, 'html5lib')
        trs = soup.find_all('tr')
        return [f"{tr.find_all('th')[0].text.strip()}:{tr.find_all('th')[1].text.strip()}"
                for tr in trs[1:]]

    @staticmethod
    def parse_fpslist(html: str) -> List[str]:
        """è§£æfpsListæ ¼å¼JSONï¼Œæå–ip+port"""
        pattern = r'const fpsList = (\[.*?\]);'
        match = re.search(pattern, html, re.DOTALL)
        if not match:
            print("âš ï¸ æœªæ‰¾åˆ°fpsListæ•°æ®")
            return []
        try:
            proxy_list = json.loads(match.group(1))
            result = []
            for item in proxy_list:
                if "ip" in item and "port" in item:
                    result.append(f"{item['ip']}:{item['port']}")
            return result
        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æå¤±è´¥ï¼š{str(e)[:50]}")
            return []

    def _get_auto_page_count(self, url: str) -> int:
        """å†…éƒ¨æ–¹æ³•ï¼šè‡ªåŠ¨è·å–api2æ€»é¡µæ•°"""
        first_page_url = url + "1" if url.endswith("page=") else url
        html = self.fetch(first_page_url)
        if not html:
            print("âŒ è‡ªåŠ¨è·å–æ€»é¡µæ•°å¤±è´¥")
            return 1
        try:
            data = json.loads(html)
            page_count = 0
            if isinstance(data.get("data"), dict):
                page_count = data["data"].get("page_count",
                                              data["data"].get("total_pages",
                                                               data["data"].get("total_page", 0)))
            page_count = page_count or data.get("page_count", 0)
            return page_count if page_count > 0 else 1
        except Exception:
            print("âŒ è§£ææ€»é¡µæ•°å¤±è´¥")
            return 1

    def crawl(self, source: dict) -> int:
        """çˆ¬å–æŒ‡å®šæºä»£ç†IP"""
        parser = getattr(self, f"parse_{source['parser']}", None)
        if not parser:
            print(f"âŒ æœªçŸ¥è§£æå™¨ï¼š{source['parser']}")
            return 0

        crawl_count = 0
        try:
            # å¤„ç†pagesï¼šæ”¯æŒ "auto" / å‡½æ•° / å›ºå®šæ•°å€¼
            if source['pages'] == "auto":
                # è‡ªåŠ¨è·å–æ€»é¡µæ•°ï¼Œä»…api2æ”¯æŒ
                if source['parser'] != "api2":
                    print("âš ï¸ ä»…api2æ”¯æŒpages='auto'")
                    total_pages = 1
                else:
                    total_pages = self._get_auto_page_count(source['url'])
                    print(f"ğŸ” è‡ªåŠ¨è·å–æ€»é¡µæ•°ï¼š{total_pages}")
            elif callable(source['pages']):
                # æ”¯æŒå‡½æ•°è·å–æ€»é¡µæ•°
                total_pages = source['pages'](source['url'])
                print(f"ğŸ” å‡½æ•°è·å–æ€»é¡µæ•°ï¼š{total_pages}")
            else:
                # å›ºå®šæ•°å€¼æ€»é¡µæ•°
                total_pages = source['pages']

            # åˆ†é¡µçˆ¬å–
            for current_page in range(1, total_pages + 1):
                if source['parser'] == "api2":
                    if "page=" in source['url']:
                        if source['url'].endswith("page="):
                            url = source['url'] + str(current_page)
                        else:
                            url = re.sub(r'page=\d+', f'page={current_page}', source['url'])
                    else:
                        url = f"{source['url']}&page={current_page}"
                elif "api1" in source['parser']:
                    url = source['url']
                elif "kxdaili.com" in source['url'] or "qiyunip.com" in source['url']:
                    url = f"{source['url']}{current_page}.html"
                else:
                    url = f"{source['url']}{current_page}"

                print(f"\nğŸ” æ­£åœ¨çˆ¬å– {source['name']} | é¡µç ï¼š{current_page}/{total_pages} | URLï¼š{url}")
                html = self.fetch(url)

                if current_page == 1:
                    print(f"ğŸ“„ å“åº”é¢„è§ˆï¼š{html[:500]}...")
                if not html:
                    print(f"âš ï¸ ç¬¬{current_page}é¡µä¸ºç©ºï¼Œè·³è¿‡")
                    time.sleep(source['delay'])
                    continue

                ips = parser(html)
                print(f"âœ… ç¬¬{current_page}é¡µæå–åˆ° {len(ips)} ä¸ªIP")
                self.proxies.extend(ips)
                crawl_count += len(ips)

                time.sleep(source['delay'])

        except Exception as e:
            print(f"âŒ çˆ¬å–å¤±è´¥ {source['name']}ï¼š{str(e)[:50]}")

        print(f"\nğŸ” {source['name']} çˆ¬å–å®Œæˆ | æ€»è®¡ï¼š{crawl_count} ä¸ªIP")
        return crawl_count

    def get_unique_proxies(self) -> List[str]:
        """ä»£ç†IPå»é‡"""
        unique_list = list(set(self.proxies))
        self.proxies.clear()
        return unique_list
